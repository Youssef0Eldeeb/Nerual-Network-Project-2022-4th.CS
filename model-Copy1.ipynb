{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257dbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libiray\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dc58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization (Data Preprocessing)\n",
    "def cleanText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^\\w\\s]','',text)\n",
    "    \n",
    "    #Remove spaces at the beginning and at the end of the string\n",
    "    text.strip()\n",
    "    \n",
    "    txt=[]\n",
    "    for w in text.split():\n",
    "        stemWord = porter.stem(w)\n",
    "        txt.append(stemWord)\n",
    "    txt = ' '.join(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee6a697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the datasets (training - testing - validation)\n",
    "train_df = pd.read_csv('Dataset/train.csv')\n",
    "test_df = pd.read_csv('Dataset/test.csv')\n",
    "val_df = pd.read_csv('Dataset/val.csv')\n",
    "# train_df.head()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1a1c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jason blake of the islanders will miss the res...</td>\n",
       "      <td>blake missing rest of season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the u.s. military on wednesday captured a wife...</td>\n",
       "      <td>u.s. arrests wife and daughter of saddam deput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>craig bellamy 's future at west ham appeared i...</td>\n",
       "      <td>west ham drops bellamy amid transfer turmoil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cambridge - when barack obama sought advice be...</td>\n",
       "      <td>in search for expertise harvard looms large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wall street held on to steep gains on monday ,...</td>\n",
       "      <td>wall street ends a three-day losing streak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  jason blake of the islanders will miss the res...   \n",
       "1  the u.s. military on wednesday captured a wife...   \n",
       "2  craig bellamy 's future at west ham appeared i...   \n",
       "3  cambridge - when barack obama sought advice be...   \n",
       "4  wall street held on to steep gains on monday ,...   \n",
       "\n",
       "                                             summary  \n",
       "0                       blake missing rest of season  \n",
       "1  u.s. arrests wife and daughter of saddam deput...  \n",
       "2       west ham drops bellamy amid transfer turmoil  \n",
       "3        in search for expertise harvard looms large  \n",
       "4         wall street ends a three-day losing streak  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine data from the three CSV files into a single DataFrame\n",
    "pre = pd.DataFrame()\n",
    "pre['text'] = pd.concat([train_df['document'], val_df['document'], test_df['document']], ignore_index=True)\n",
    "pre['summary'] = pd.concat([train_df['summary'], val_df['summary'], test_df['summary']], ignore_index=True)\n",
    "pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1485b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre['text'] = pre['text'].apply(cleanText)\n",
    "pre['summary'] = pre['summary'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77e3083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.shape\n",
    "# pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3833ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       0\n",
       "summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "pre.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfd0af",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee901582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248dfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text to get the vocab count \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "x_tokenizer = Tokenizer(lower=True, split=' ') \n",
    "x_tokenizer.fit_on_texts(pre['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5a46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d237deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3321, 8048, 3, 1, 348, 37, 601, 1, 1358, 3, 1, 323, 555, 51, 425, 39, 13, 19, 969, 53, 20, 9750, 1210, 6, 28, 4, 415, 2116, 964]\n"
     ]
    }
   ],
   "source": [
    "X = x_tokenizer.texts_to_sequences(pre['text'].values)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da22c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 3321 8048    3\n",
      "    1  348   37  601    1 1358    3    1  323  555   51  425   39   13\n",
      "   19  969   53   20 9750 1210    6   28    4  415 2116  964]\n"
     ]
    }
   ],
   "source": [
    "# make all text same size\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48df0edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 68)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b585bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6cedf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text to get the vocab count \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "y_tokenizer = Tokenizer(lower=True, split=' ') \n",
    "y_tokenizer.fit_on_texts(pre['summary'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154ddf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1844a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 64, 854, 18, 1124, 3, 831, 703, 130, 582, 4, 4588]\n"
     ]
    }
   ],
   "source": [
    "Y = y_tokenizer.texts_to_sequences(pre['summary'].values)\n",
    "print(Y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21d563e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    7\n",
      "   64  854   18 1124    3  831  703  130  582    4 4588]\n"
     ]
    }
   ],
   "source": [
    "# make all text same size\n",
    "Y = pad_sequences(Y)\n",
    "print(Y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a92dea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ad49c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0, 4587,  227,\n",
       "        1696,    3,  796],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    7,   64,  854,   18, 1124,    3,  831,  703,  130,\n",
       "         582,    4, 4588],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,  439, 3947,  223, 7425,\n",
       "         242, 1201, 2481],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    2,  505,    4, 5545,\n",
       "        5546, 1202, 1385]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y = pd.get_dummies(pre['summary'])\n",
    "# Y[0]\n",
    "Y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "513e31de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17600, 68) (17600, 25)\n",
      "(4400, 68) (4400, 25)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e24021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3503ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Check on the most number of word in text\n",
    "# Check how much % of text have 0-60 words\n",
    "cnt = 0\n",
    "for i in pre['text']:\n",
    "    if len(i.split()) <= 70:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(pre['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c3db6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997272727272727\n"
     ]
    }
   ],
   "source": [
    "# Check on the most number of word in summary\n",
    "# Check how much % of summary have 0-20 words\n",
    "cnt = 0\n",
    "for i in pre['summary']:\n",
    "    if len(i.split()) <= 20:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(pre['summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89ebfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to summarize the text between 0-20 words for Summary and 0-70 words for Text\n",
    "max_text_len = 70\n",
    "max_summary_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf24775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb41a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d2dc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the Summaries and Text which fall below max length \n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# cleaned_text = np.array(pre['text'])\n",
    "# cleaned_summary = np.array(pre['summary'])\n",
    "\n",
    "# short_text = []\n",
    "# short_summary = []\n",
    "\n",
    "# for i in range(len(train_df)):\n",
    "#     if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
    "#         short_text.append(cleaned_text[i])\n",
    "#         short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "# post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
    "\n",
    "# post_pre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0f13344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add sostok(start of the sequence) and eostok(end of the sequence)\n",
    "\n",
    "# post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \\\n",
    "#         + ' eostok')\n",
    "\n",
    "# post_pre.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45338c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the data into train and test data chunks.\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train ,x_val, y_train, y_val = train_test_split(\n",
    "#     np.array(post_pre[\"text\"]),\n",
    "#     np.array(post_pre[\"summary\"]),\n",
    "#     test_size=0.1,\n",
    "#     random_state=0,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8b17446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize the text to get the vocab count \n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Prepare a tokenizer on training data\n",
    "# x_tokenizer = Tokenizer() \n",
    "# x_tokenizer.fit_on_texts(list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9c92a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Find the percentage of occurrence of rare words (say, occurring less than 5 times) in the text.\n",
    "# thresh = 5\n",
    "\n",
    "# cnt = 0\n",
    "# tot_cnt = 0\n",
    "\n",
    "# for key, value in x_tokenizer.word_counts.items():\n",
    "#     tot_cnt = tot_cnt + 1\n",
    "#     if value < thresh:\n",
    "#         cnt = cnt + 1\n",
    "    \n",
    "# print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)\n",
    "\n",
    "# # Prepare a tokenizer, again -- by not considering the rare words\n",
    "# x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "# x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "# # Convert text sequences to integer sequences \n",
    "# x_tr_seq = x_tokenizer.texts_to_sequences(x_train) \n",
    "# x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# # Pad zero upto maximum length\n",
    "# x_train = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "# x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# # Size of vocabulary (+1 for padding token)\n",
    "# x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "# print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "799d99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare a tokenizer on testing data\n",
    "# y_tokenizer = Tokenizer()   \n",
    "# y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# thresh = 5\n",
    "\n",
    "# cnt = 0\n",
    "# tot_cnt = 0\n",
    "\n",
    "# for key, value in y_tokenizer.word_counts.items():\n",
    "#     tot_cnt = tot_cnt + 1\n",
    "#     if value < thresh:\n",
    "#         cnt = cnt + 1\n",
    "    \n",
    "# print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "# # Prepare a tokenizer, again -- by not considering the rare words\n",
    "# y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "# y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# # Convert text sequences to integer sequences \n",
    "# y_tr_seq = y_tokenizer.texts_to_sequences(y_train) \n",
    "# y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# # Pad zero upto maximum length\n",
    "# y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "# y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# # Size of vocabulary (+1 for padding token)\n",
    "# y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "# print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a336c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8c445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d7d515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7916d543",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_voc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/f9_zspms2z3cctnc45ym9f040000gn/T/ipykernel_5113/4035635043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_voc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Encoder LSTM 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_voc' is not defined"
     ]
    }
   ],
   "source": [
    "latent_dim = 300\n",
    "embedding_dim = 200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb = Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using encoder_states as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
    "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a0d5b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7da922e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/f9_zspms2z3cctnc45ym9f040000gn/T/ipykernel_763/940575561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# history = model.fit(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     [x_train, y_train[:, :-1]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# y_train\n",
    "y_train = y_train[:, -1].reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "y_train\n",
    "# history = model.fit(\n",
    "#     [x_train, y_train[:, :-1]],\n",
    "#     y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n",
    "#     epochs=50,\n",
    "#     callbacks=[es],\n",
    "#     batch_size=128,\n",
    "#     validation_data=([x_val, y_val[:, :-1]],\n",
    "#                      y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
    "#                      , 1:]),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71607be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot\n",
    "\n",
    "# pyplot.plot(history.history['loss'], label='train')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
